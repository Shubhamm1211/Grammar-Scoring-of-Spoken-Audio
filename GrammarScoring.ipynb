{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHM5rfXycG_8"
      },
      "outputs": [],
      "source": [
        "!unzip -q dataset.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xGKxqMBfk6Z",
        "outputId": "f2bfb10f-aad8-4d3d-de3c-1bd091de6c3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m836.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install librosa transformers datasets torchaudio --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDgXBn7Nf1rT",
        "outputId": "052fba92-cdbf-4e21-d2db-e308ed7a82b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "100%|██████████| 444/444 [4:59:30<00:00, 40.47s/it]\n",
            "<ipython-input-8-6dc8c53450e5>:37: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  X_train = torch.tensor(X_train)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import librosa\n",
        "import pandas as pd\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load model & processor\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model.eval()  # Inference mode\n",
        "\n",
        "def extract_wav2vec_features(file_path):\n",
        "    speech, sr = librosa.load(file_path, sr=16000)  # Load and resample to 16kHz\n",
        "    input_values = processor(speech, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
        "    with torch.no_grad():\n",
        "        features = model(input_values).last_hidden_state  # (1, time_steps, feature_dim)\n",
        "    # Take mean across time dimension to get a fixed-length vector\n",
        "    return features.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "# Load training CSV\n",
        "train_df = pd.read_csv(\"dataset/train.csv\")\n",
        "\n",
        "# Path to audios\n",
        "audio_folder = \"dataset/audios_train\"\n",
        "\n",
        "# Extract features for each audio file\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "for i, row in tqdm(train_df.iterrows(), total=len(train_df)):\n",
        "    audio_file = os.path.join(audio_folder, row[\"filename\"])\n",
        "    features = extract_wav2vec_features(audio_file)\n",
        "    X_train.append(features)\n",
        "    y_train.append(row[\"label\"])  # assuming label column is 'score'\n",
        "\n",
        "X_train = torch.tensor(X_train)\n",
        "y_train = torch.tensor(y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdqGoOjIgZ3Z",
        "outputId": "34a75ebc-8bd1-46f1-b99a-c9e823de40a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['filename', 'label'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(\"dataset/train.csv\")\n",
        "print(train_df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tw-lU6Y4hKjZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X_train.numpy(), dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y_train.numpy(), dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Train-validation split\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create Datasets and Loaders\n",
        "train_dataset = torch.utils.data.TensorDataset(X_tr, y_tr)\n",
        "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhzGjNg3ovqX"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MLPRegressor(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MLPRegressor, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh5NMB2Io0NQ"
      },
      "outputs": [],
      "source": [
        "model = MLPRegressor(input_dim=X_tensor.shape[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AIeR72Ho27i",
        "outputId": "ccab89f8-898a-4397-de8e-71c7b35cb486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Epoch 01 | Train Loss: 75.7969 |  Pearson: -0.1340\n",
            " Epoch 02 | Train Loss: 26.7540 |  Pearson: -0.1225\n",
            " Epoch 03 | Train Loss: 17.6739 |  Pearson: -0.1068\n",
            " Epoch 04 | Train Loss: 15.1165 |  Pearson: -0.0805\n",
            " Epoch 05 | Train Loss: 15.7304 |  Pearson: -0.0615\n",
            " Epoch 06 | Train Loss: 14.7131 |  Pearson: -0.0244\n",
            " Epoch 07 | Train Loss: 15.3675 |  Pearson: 0.0129\n",
            " Epoch 08 | Train Loss: 14.1034 |  Pearson: 0.0564\n",
            " Epoch 09 | Train Loss: 14.2243 |  Pearson: 0.1486\n",
            " Epoch 10 | Train Loss: 13.5105 |  Pearson: 0.1762\n",
            " Epoch 11 | Train Loss: 12.7650 |  Pearson: 0.2413\n",
            " Epoch 12 | Train Loss: 12.9179 |  Pearson: 0.2924\n",
            " Epoch 13 | Train Loss: 11.5960 |  Pearson: 0.3577\n",
            " Epoch 14 | Train Loss: 12.7501 |  Pearson: 0.4313\n",
            " Epoch 15 | Train Loss: 13.2081 |  Pearson: 0.4445\n",
            " Epoch 16 | Train Loss: 11.5714 |  Pearson: 0.4648\n",
            " Epoch 17 | Train Loss: 10.2365 |  Pearson: 0.4633\n",
            " Epoch 18 | Train Loss: 10.3440 |  Pearson: 0.4799\n",
            " Epoch 19 | Train Loss: 10.1568 |  Pearson: 0.5029\n",
            " Epoch 20 | Train Loss: 9.4048 |  Pearson: 0.5065\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        preds = model(xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_preds = model(X_val).squeeze().numpy()\n",
        "        val_true = y_val.squeeze().numpy()\n",
        "        pearson_corr, _ = pearsonr(val_preds, val_true)\n",
        "\n",
        "    print(f\" Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Pearson: {pearson_corr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pAzRoUXpiJp"
      },
      "outputs": [],
      "source": [
        "def load_audio(file_path):\n",
        "    waveform, sr = torchaudio.load(file_path)\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = waveform.mean(dim=0, keepdim=True)  # convert to mono\n",
        "    return waveform.squeeze()  # remove channel dim if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rXzsXikqiNh"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FBGkhq9qoRA",
        "outputId": "3164a69f-6ddc-435c-f681-32365a79114d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['masked_spec_embed']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Wav2Vec2Model(\n",
              "  (feature_extractor): Wav2Vec2FeatureEncoder(\n",
              "    (conv_layers): ModuleList(\n",
              "      (0): Wav2Vec2GroupNormConvLayer(\n",
              "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
              "        (activation): GELUActivation()\n",
              "        (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "      )\n",
              "      (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
              "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
              "        (activation): GELUActivation()\n",
              "      )\n",
              "      (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
              "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
              "        (activation): GELUActivation()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (feature_projection): Wav2Vec2FeatureProjection(\n",
              "    (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    (projection): Linear(in_features=512, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): Wav2Vec2Encoder(\n",
              "    (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
              "      (conv): ParametrizedConv1d(\n",
              "        768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
              "        (parametrizations): ModuleDict(\n",
              "          (weight): ParametrizationList(\n",
              "            (0): _WeightNorm()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (padding): Wav2Vec2SamePadLayer()\n",
              "      (activation): GELUActivation()\n",
              "    )\n",
              "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (layers): ModuleList(\n",
              "      (0-11): 12 x Wav2Vec2EncoderLayer(\n",
              "        (attention): Wav2Vec2SdpaAttention(\n",
              "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (feed_forward): Wav2Vec2FeedForward(\n",
              "          (intermediate_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "          (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
        "\n",
        "# Load processor and model\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model_wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to(device)\n",
        "model_wav2vec.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfrSSyPh4B9v"
      },
      "outputs": [],
      "source": [
        "input_values = processor(load_audio(path), sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1rviTa5o8qS",
        "outputId": "8de39bd3-fa39-4d5f-f55f-c37ad4aeecc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No saved file found. Starting from scratch.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "try:\n",
        "    test_embeddings = list(np.load(\"test_embeddings_partial.npy\", allow_pickle=True))\n",
        "    print(f\"Loaded {len(test_embeddings)} embeddings.\")\n",
        "except:\n",
        "    test_embeddings = []\n",
        "    print(\"No saved file found. Starting from scratch.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLLtyx5NpKju",
        "outputId": "7ac6658a-f202-40b6-d5e3-a0e4bd2315fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming from index: 0\n"
          ]
        }
      ],
      "source": [
        "start_idx = len(test_embeddings)\n",
        "print(f\"Resuming from index: {start_idx}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6PrTUE9p_zX",
        "outputId": "066a5f0f-fe73-4cd5-9794-e5bf01fac6a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 57%|█████▋    | 112/195 [55:59<39:10, 28.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Skipping audio_159.wav: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 2, 2915328]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 195/195 [2:00:18<00:00, 37.02s/it]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "import torchaudio\n",
        "\n",
        "# Helper to load audio\n",
        "def load_audio(file_path):\n",
        "    waveform, sample_rate = torchaudio.load(file_path)\n",
        "    if sample_rate != 16000:\n",
        "        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)\n",
        "    return waveform.squeeze()\n",
        "\n",
        "# Use GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_wav2vec = model_wav2vec.to(device)\n",
        "\n",
        "# List to store successful results\n",
        "test_embeddings = []\n",
        "valid_files = []\n",
        "\n",
        "# Loop through test files\n",
        "for file in tqdm(test_df['filename']):\n",
        "    path = os.path.join(\"dataset/audios_test\", file)\n",
        "    try:\n",
        "        waveform = load_audio(path)\n",
        "        input_values = processor(waveform, sampling_rate=16000, return_tensors=\"pt\").input_values.to(device)\n",
        "        with torch.no_grad():\n",
        "            embedding = model_wav2vec(input_values).last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "        test_embeddings.append(embedding)\n",
        "        valid_files.append(file)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {file}: {e}\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "X_test = pd.DataFrame(test_embeddings)\n",
        "test_filenames = valid_files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnq69TBK43ST"
      },
      "outputs": [],
      "source": [
        "# Convert to tensor and predict\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    preds = model(X_test_tensor).squeeze().cpu().numpy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhaSyE2_hbe9"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame with filename and predicted score\n",
        "submission_df = pd.DataFrame({\n",
        "    \"filename\": test_filenames,\n",
        "    \"mos\": preds\n",
        "})\n",
        "\n",
        "# Just to be safe, round the scores between 0 to 5\n",
        "submission_df[\"mos\"] = submission_df[\"mos\"].clip(0, 5)\n",
        "\n",
        "# Save to CSV\n",
        "submission_df.to_csv(\"submission.csv\", index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
